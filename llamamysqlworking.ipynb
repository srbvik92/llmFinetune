{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02717ddc",
   "metadata": {},
   "source": [
    "First of all, so this to make sure that your device won't crash in middle of trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "599a3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f9ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc24a9bc",
   "metadata": {},
   "source": [
    "Login into huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f0807e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa17f1b6aacb422586745f4ad5396076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6595e4",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46cbc978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: C:\\Users\\saura\\.conda\\envs\\gpt1\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary C:\\Users\\saura\\.conda\\envs\\gpt1\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: saura\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, AutoTokenizer\n",
    "\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, LoraConfig, PeftModel\n",
    "\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a905c",
   "metadata": {},
   "source": [
    "All the variables define them, description is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724be278",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config= BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_use_double_quant=True,\n",
    "        bnb_8bit_quant_type=\"nf4\",\n",
    "        bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        llm_int8_skip_modules= ['decoder', 'lm_head',  'wo'],\n",
    "    )\n",
    "\n",
    "# base model name\n",
    "model_id = 'meta-llama/Llama-2-7b-hf';\n",
    "\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"sqlModel\"\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# output directory for model\n",
    "output_dir = \"trained1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ebabc",
   "metadata": {},
   "source": [
    "Get the maximum length of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1676a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "  max_length = None\n",
    "  for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "    max_length = getattr(model.config, length_setting, None)\n",
    "    if max_length:\n",
    "      logger.info(f\"Found max lenth: {max_length}\")\n",
    "      break\n",
    "  if not max_length:\n",
    "    max_length = 32_000\n",
    "    logger.info(f\"Using default max length: {max_length}\")\n",
    "  return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53765a90",
   "metadata": {},
   "source": [
    "Load the model, tokenizer and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43aa82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             trust_remote_code=True, \n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             load_in_8bit=True,\n",
    "                                             )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
    "                                          use_fast = False,\n",
    "                                          max_length=get_max_length(),\n",
    "                                          )\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "dataset = load_dataset('b-mc2/sql-create-context', split=\"train\")\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c844ca",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b2400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        r=32,\n",
    "        lora_dropout=0.1,\n",
    "        \n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\", \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "max_seq_length = get_max_length() # max sequence length for model and packing of the dataset\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dirs,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size= 12, #6 if use_flash_attention else 3,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    #optim=\"paged_adamw_32bit\",\n",
    "    \n",
    "    #torch_compile=True, # optimizations\n",
    "    #optim=\"adamw_torch_fused\", # improved optimizer \n",
    "    #optim=\"adamw_bnb_8bit\", #     #['adamw_hf', 'adamw_torch', 'adamw_torch_fused', 'adamw_torch_xla', 'adamw_apex_fused', 'adafactor', 'adamw_bnb_8bit', 'adamw_anyprecision', 'sgd', 'adagrad']\n",
    "\n",
    "    save_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    # fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm= False, #True # disable tqdm since with packing values are in correct\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903289c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78fe61d",
   "metadata": {},
   "source": [
    "Now this is really important function, need to format the dataset otherwise it won't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c0fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['context'])):\n",
    "        print(i)\n",
    "        text = f\"### Question: {example['question'][i]}\\n ### Answer: {example['answer'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "response_template = \" ### Answer:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length, \n",
    "    tokenizer=tokenizer,\n",
    "    formatting_func=formatting_prompts_func, \n",
    "    data_collator = collator,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e20591",
   "metadata": {},
   "source": [
    "So, if training for the first time, set resume_from_checkpoint = False, otherwise it'll throw error, and then set it true once it has saved a checkpoint, it'll save time for retraining if the system crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c2559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 6: Train the model\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb2223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Save the model\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62267acd",
   "metadata": {},
   "source": [
    "Now reload the model, restart the kernel before doing it, if just done training, then skip the next cell and do the inference directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c32ee4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fccaadbaf43448a8d6bfb0bc7e44d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb180159",
   "metadata": {},
   "source": [
    "Need to make sure both the model and the prompt are loaded in the same device, i.e. gpu here in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46190e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(model.parameters()).device)\n",
    "\n",
    "prompt = \"What is a sql query?\"  # adjust this based on your requirements\n",
    "encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "encoded_prompt = encoded_prompt.cuda()\n",
    "\n",
    "print(encoded_prompt.device)  # Should output cuda:X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15fb1b3",
   "metadata": {},
   "source": [
    "Do the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7936a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sequence from the provided prompt\n",
    "output_sequence = model.generate(\n",
    "    encoded_prompt,\n",
    "    max_length=100,  # adjust as needed\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "generated_sequence = output_sequence[:, len(encoded_prompt[0]):][0]\n",
    "decoded_sequence = tokenizer.decode(generated_sequence, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a00039",
   "metadata": {},
   "source": [
    "Print the inference, ie output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f0ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f75934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
